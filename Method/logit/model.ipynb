{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>...</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>day_of_week_fri</th>\n",
       "      <th>day_of_week_mon</th>\n",
       "      <th>day_of_week_thu</th>\n",
       "      <th>day_of_week_tue</th>\n",
       "      <th>day_of_week_wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171429</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882307</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882307</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.389322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  default  housing  loan  campaign  pdays  previous  poutcome  \\\n",
       "0  0.171429        1       -1     1  0.029412    1.0       0.0         0   \n",
       "1  0.300000        1        1     1  0.088235    1.0       0.0         0   \n",
       "2  0.100000        1       -1     1  0.000000    1.0       0.0         0   \n",
       "3  0.285714        1        0     0  0.058824    1.0       0.0         0   \n",
       "4  0.414286        1       -1     1  0.000000    1.0       0.0         0   \n",
       "\n",
       "   emp.var.rate  cons.price.idx  ...  month_mar  month_may  month_nov  \\\n",
       "0      0.333333        0.269680  ...          0          1          0   \n",
       "1      0.937500        0.698753  ...          0          1          0   \n",
       "2      1.000000        0.882307  ...          0          0          0   \n",
       "3      1.000000        0.882307  ...          0          0          0   \n",
       "4      0.687500        0.389322  ...          0          0          1   \n",
       "\n",
       "   month_oct  month_sep  day_of_week_fri  day_of_week_mon  day_of_week_thu  \\\n",
       "0          0          0                1                0                0   \n",
       "1          0          0                1                0                0   \n",
       "2          0          0                0                0                0   \n",
       "3          0          0                1                0                0   \n",
       "4          0          0                0                1                0   \n",
       "\n",
       "   day_of_week_tue  day_of_week_wed  \n",
       "0                0                0  \n",
       "1                0                0  \n",
       "2                0                1  \n",
       "3                0                0  \n",
       "4                0                0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"..\\..\\Data\\small_ohe.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'default', 'housing', 'loan', 'campaign', 'pdays', 'previous',\n",
       "       'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
       "       'euribor3m', 'nr.employed', 'y', 'pdays2', 'job_admin.',\n",
       "       'job_blue-collar', 'job_entrepreneur', 'job_housemaid',\n",
       "       'job_management', 'job_retired', 'job_self-employed', 'job_services',\n",
       "       'job_student', 'job_technician', 'job_unemployed', 'job_unknown',\n",
       "       'marital_divorced', 'marital_married', 'marital_single',\n",
       "       'marital_unknown', 'education_basic.4y', 'education_basic.6y',\n",
       "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
       "       'education_professional.course', 'education_university.degree',\n",
       "       'education_unknown', 'contact_cellular', 'contact_telephone',\n",
       "       'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun',\n",
       "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep',\n",
       "       'day_of_week_fri', 'day_of_week_mon', 'day_of_week_thu',\n",
       "       'day_of_week_tue', 'day_of_week_wed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saperating features and result vectors\n",
    "X = data.drop('y', axis=1).values\n",
    "y = data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define no sequence of layers\n",
    "input_tensor = tf.keras.Input(shape=(55,))\n",
    "layer1 = tf.keras.layers.Dense(64,name = 'layer_1', activation='relu',use_bias=True)(input_tensor)\n",
    "layer2 = tf.keras.layers.Dense(64,name = 'layer_2', activation='relu',use_bias=True)(layer1)\n",
    "layer3 = tf.keras.layers.Dense(32,name = 'layer_3', activation='relu',use_bias=True)(layer2)\n",
    "layer4 = tf.keras.layers.Dense(1, name = 'logit', activation=None,use_bias=True)(layer3)\n",
    "output_tensor = tf.keras.layers.Activation('sigmoid', name='output')(layer4)\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=[layer1,layer2,layer3,layer4,output_tensor]) \n",
    "losses = {'logit': 'BinaryCrossentropy'}\n",
    "model.compile(loss=losses, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 0.6446 - logit_loss: 0.6446 - layer_1_accuracy: 0.0984 - layer_2_accuracy: 6.1087e-04 - layer_3_accuracy: 0.1365 - logit_accuracy: 0.6423 - output_accuracy: 0.4953 - val_loss: 0.5566 - val_logit_loss: 0.5566 - val_layer_1_accuracy: 0.1113 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1264 - val_logit_accuracy: 0.7459 - val_output_accuracy: 0.4973\n",
      "Epoch 2/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5507 - logit_loss: 0.5507 - layer_1_accuracy: 0.0895 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1692 - logit_accuracy: 0.7268 - output_accuracy: 0.4959 - val_loss: 0.5381 - val_logit_loss: 0.5381 - val_layer_1_accuracy: 0.1016 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1854 - val_logit_accuracy: 0.7569 - val_output_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5054 - logit_loss: 0.5054 - layer_1_accuracy: 0.0924 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2019 - logit_accuracy: 0.7546 - output_accuracy: 0.5002 - val_loss: 0.5712 - val_logit_loss: 0.5712 - val_layer_1_accuracy: 0.1140 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1923 - val_logit_accuracy: 0.7734 - val_output_accuracy: 0.5096\n",
      "Epoch 4/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4664 - logit_loss: 0.4664 - layer_1_accuracy: 0.0993 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2356 - logit_accuracy: 0.7703 - output_accuracy: 0.5156 - val_loss: 0.5026 - val_logit_loss: 0.5026 - val_layer_1_accuracy: 0.1168 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2033 - val_logit_accuracy: 0.7788 - val_output_accuracy: 0.5137\n",
      "Epoch 5/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4433 - logit_loss: 0.4433 - layer_1_accuracy: 0.1103 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2204 - logit_accuracy: 0.7926 - output_accuracy: 0.5521 - val_loss: 0.4754 - val_logit_loss: 0.4754 - val_layer_1_accuracy: 0.1401 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2225 - val_logit_accuracy: 0.7953 - val_output_accuracy: 0.5440\n",
      "Epoch 6/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4043 - logit_loss: 0.4043 - layer_1_accuracy: 0.1161 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2442 - logit_accuracy: 0.8080 - output_accuracy: 0.5783 - val_loss: 0.4477 - val_logit_loss: 0.4477 - val_layer_1_accuracy: 0.1346 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2143 - val_logit_accuracy: 0.8022 - val_output_accuracy: 0.5714\n",
      "Epoch 7/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4125 - logit_loss: 0.4125 - layer_1_accuracy: 0.0976 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2741 - logit_accuracy: 0.8143 - output_accuracy: 0.5889 - val_loss: 0.4325 - val_logit_loss: 0.4325 - val_layer_1_accuracy: 0.1181 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2555 - val_logit_accuracy: 0.8118 - val_output_accuracy: 0.5989\n",
      "Epoch 8/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3667 - logit_loss: 0.3667 - layer_1_accuracy: 0.1112 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2547 - logit_accuracy: 0.8320 - output_accuracy: 0.6249 - val_loss: 0.4459 - val_logit_loss: 0.4459 - val_layer_1_accuracy: 0.1332 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2266 - val_logit_accuracy: 0.8269 - val_output_accuracy: 0.6181\n",
      "Epoch 9/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3356 - logit_loss: 0.3356 - layer_1_accuracy: 0.1087 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2557 - logit_accuracy: 0.8586 - output_accuracy: 0.6726 - val_loss: 0.4611 - val_logit_loss: 0.4611 - val_layer_1_accuracy: 0.1346 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2143 - val_logit_accuracy: 0.8269 - val_output_accuracy: 0.6291\n",
      "Epoch 10/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2962 - logit_loss: 0.2962 - layer_1_accuracy: 0.1138 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2440 - logit_accuracy: 0.8763 - output_accuracy: 0.6975 - val_loss: 0.4554 - val_logit_loss: 0.4554 - val_layer_1_accuracy: 0.1346 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2129 - val_logit_accuracy: 0.8475 - val_output_accuracy: 0.6841\n",
      "Epoch 11/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2783 - logit_loss: 0.2783 - layer_1_accuracy: 0.1119 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2338 - logit_accuracy: 0.8929 - output_accuracy: 0.7242 - val_loss: 0.3979 - val_logit_loss: 0.3979 - val_layer_1_accuracy: 0.1319 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2239 - val_logit_accuracy: 0.8434 - val_output_accuracy: 0.7294\n",
      "Epoch 12/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2563 - logit_loss: 0.2563 - layer_1_accuracy: 0.1095 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2280 - logit_accuracy: 0.9016 - output_accuracy: 0.7517 - val_loss: 0.4188 - val_logit_loss: 0.4188 - val_layer_1_accuracy: 0.1140 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2102 - val_logit_accuracy: 0.8626 - val_output_accuracy: 0.7266\n",
      "Epoch 13/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3014 - logit_loss: 0.3014 - layer_1_accuracy: 0.1075 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2321 - logit_accuracy: 0.8989 - output_accuracy: 0.7700 - val_loss: 0.4248 - val_logit_loss: 0.4248 - val_layer_1_accuracy: 0.1181 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2239 - val_logit_accuracy: 0.8503 - val_output_accuracy: 0.6772\n",
      "Epoch 14/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3037 - logit_loss: 0.3037 - layer_1_accuracy: 0.1093 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2202 - logit_accuracy: 0.8954 - output_accuracy: 0.7764 - val_loss: 0.8553 - val_logit_loss: 0.8553 - val_layer_1_accuracy: 0.1456 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1648 - val_logit_accuracy: 0.8159 - val_output_accuracy: 0.6277\n",
      "Epoch 15/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3558 - logit_loss: 0.3558 - layer_1_accuracy: 0.0945 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.2233 - logit_accuracy: 0.8830 - output_accuracy: 0.7363 - val_loss: 0.3940 - val_logit_loss: 0.3940 - val_layer_1_accuracy: 0.1030 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.2170 - val_logit_accuracy: 0.8668 - val_output_accuracy: 0.7074\n",
      "Epoch 16/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2357 - logit_loss: 0.2357 - layer_1_accuracy: 0.0809 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.2176 - logit_accuracy: 0.9201 - output_accuracy: 0.7783 - val_loss: 0.3592 - val_logit_loss: 0.3592 - val_layer_1_accuracy: 0.1071 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1717 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.7582\n",
      "Epoch 17/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4911 - logit_loss: 0.4911 - layer_1_accuracy: 0.0803 - layer_2_accuracy: 9.1631e-04 - layer_3_accuracy: 0.1634 - logit_accuracy: 0.8775 - output_accuracy: 0.7740 - val_loss: 0.8995 - val_logit_loss: 0.8995 - val_layer_1_accuracy: 0.0591 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1058 - val_logit_accuracy: 0.8077 - val_output_accuracy: 0.6882\n",
      "Epoch 18/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5899 - logit_loss: 0.5899 - layer_1_accuracy: 0.0644 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.1260 - logit_accuracy: 0.8534 - output_accuracy: 0.7075 - val_loss: 0.5482 - val_logit_loss: 0.5482 - val_layer_1_accuracy: 0.0797 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1593 - val_logit_accuracy: 0.8242 - val_output_accuracy: 0.7074\n",
      "Epoch 19/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4049 - logit_loss: 0.4049 - layer_1_accuracy: 0.0741 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.1743 - logit_accuracy: 0.8801 - output_accuracy: 0.7063 - val_loss: 0.3948 - val_logit_loss: 0.3948 - val_layer_1_accuracy: 0.0810 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1717 - val_logit_accuracy: 0.8530 - val_output_accuracy: 0.7473\n",
      "Epoch 20/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3260 - logit_loss: 0.3260 - layer_1_accuracy: 0.0762 - layer_2_accuracy: 0.0023 - layer_3_accuracy: 0.1885 - logit_accuracy: 0.9004 - output_accuracy: 0.7572 - val_loss: 0.4240 - val_logit_loss: 0.4240 - val_layer_1_accuracy: 0.0797 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1964 - val_logit_accuracy: 0.8846 - val_output_accuracy: 0.7280\n",
      "Epoch 21/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2892 - logit_loss: 0.2892 - layer_1_accuracy: 0.0759 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.1965 - logit_accuracy: 0.9189 - output_accuracy: 0.7877 - val_loss: 0.3816 - val_logit_loss: 0.3816 - val_layer_1_accuracy: 0.0810 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1964 - val_logit_accuracy: 0.8887 - val_output_accuracy: 0.7720\n",
      "Epoch 22/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2703 - logit_loss: 0.2703 - layer_1_accuracy: 0.0788 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.2020 - logit_accuracy: 0.9282 - output_accuracy: 0.8134 - val_loss: 0.4036 - val_logit_loss: 0.4036 - val_layer_1_accuracy: 0.0838 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1964 - val_logit_accuracy: 0.9011 - val_output_accuracy: 0.7596\n",
      "Epoch 23/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2544 - logit_loss: 0.2544 - layer_1_accuracy: 0.0782 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.2014 - logit_accuracy: 0.9330 - output_accuracy: 0.8213 - val_loss: 0.3692 - val_logit_loss: 0.3692 - val_layer_1_accuracy: 0.0824 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1896 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.7871\n",
      "Epoch 24/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2427 - logit_loss: 0.2427 - layer_1_accuracy: 0.0762 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.1979 - logit_accuracy: 0.9391 - output_accuracy: 0.8418 - val_loss: 0.3474 - val_logit_loss: 0.3474 - val_layer_1_accuracy: 0.0824 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1923 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8036\n",
      "Epoch 25/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2251 - logit_loss: 0.2251 - layer_1_accuracy: 0.0761 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.1982 - logit_accuracy: 0.9449 - output_accuracy: 0.8491 - val_loss: 0.3778 - val_logit_loss: 0.3778 - val_layer_1_accuracy: 0.0810 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1937 - val_logit_accuracy: 0.9162 - val_output_accuracy: 0.7953\n",
      "Epoch 26/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3003 - logit_loss: 0.3003 - layer_1_accuracy: 0.0722 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.1964 - logit_accuracy: 0.9262 - output_accuracy: 0.8526 - val_loss: 0.7965 - val_logit_loss: 0.7965 - val_layer_1_accuracy: 0.0769 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1772 - val_logit_accuracy: 0.8571 - val_output_accuracy: 0.7747\n",
      "Epoch 27/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3433 - logit_loss: 0.3433 - layer_1_accuracy: 0.0690 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.1962 - logit_accuracy: 0.9189 - output_accuracy: 0.8250 - val_loss: 0.3737 - val_logit_loss: 0.3737 - val_layer_1_accuracy: 0.0783 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1937 - val_logit_accuracy: 0.9011 - val_output_accuracy: 0.7967\n",
      "Epoch 28/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2463 - logit_loss: 0.2463 - layer_1_accuracy: 0.0738 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2068 - logit_accuracy: 0.9407 - output_accuracy: 0.8522 - val_loss: 0.3686 - val_logit_loss: 0.3686 - val_layer_1_accuracy: 0.0838 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1964 - val_logit_accuracy: 0.9148 - val_output_accuracy: 0.8242\n",
      "Epoch 29/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2703 - logit_loss: 0.2703 - layer_1_accuracy: 0.0750 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.2024 - logit_accuracy: 0.9392 - output_accuracy: 0.8676 - val_loss: 0.7062 - val_logit_loss: 0.7062 - val_layer_1_accuracy: 0.0755 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1841 - val_logit_accuracy: 0.8805 - val_output_accuracy: 0.7514\n",
      "Epoch 30/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2576 - logit_loss: 0.2576 - layer_1_accuracy: 0.0680 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1918 - logit_accuracy: 0.9430 - output_accuracy: 0.8586 - val_loss: 0.4088 - val_logit_loss: 0.4088 - val_layer_1_accuracy: 0.0714 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1909 - val_logit_accuracy: 0.9162 - val_output_accuracy: 0.8173\n",
      "Epoch 31/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2291 - logit_loss: 0.2291 - layer_1_accuracy: 0.0699 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1849 - logit_accuracy: 0.9525 - output_accuracy: 0.8775 - val_loss: 0.4010 - val_logit_loss: 0.4010 - val_layer_1_accuracy: 0.0701 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1731 - val_logit_accuracy: 0.9245 - val_output_accuracy: 0.8242\n",
      "Epoch 32/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2169 - logit_loss: 0.2169 - layer_1_accuracy: 0.0675 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1784 - logit_accuracy: 0.9546 - output_accuracy: 0.8850 - val_loss: 0.4271 - val_logit_loss: 0.4271 - val_layer_1_accuracy: 0.0687 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1648 - val_logit_accuracy: 0.9217 - val_output_accuracy: 0.8420\n",
      "Epoch 33/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2089 - logit_loss: 0.2089 - layer_1_accuracy: 0.0669 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1714 - logit_accuracy: 0.9626 - output_accuracy: 0.8942 - val_loss: 0.7327 - val_logit_loss: 0.7327 - val_layer_1_accuracy: 0.0701 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1442 - val_logit_accuracy: 0.8929 - val_output_accuracy: 0.7788\n",
      "Epoch 34/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4362 - logit_loss: 0.4362 - layer_1_accuracy: 0.0632 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.1414 - logit_accuracy: 0.9230 - output_accuracy: 0.8706 - val_loss: 2.0281 - val_logit_loss: 2.0281 - val_layer_1_accuracy: 0.0426 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1978 - val_logit_accuracy: 0.8146 - val_output_accuracy: 0.8104\n",
      "Epoch 35/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.8761 - logit_loss: 1.8761 - layer_1_accuracy: 0.0316 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1704 - logit_accuracy: 0.7819 - output_accuracy: 0.7485 - val_loss: 1.4455 - val_logit_loss: 1.4455 - val_layer_1_accuracy: 0.0288 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1181 - val_logit_accuracy: 0.7775 - val_output_accuracy: 0.7747\n",
      "Epoch 36/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8810 - logit_loss: 0.8810 - layer_1_accuracy: 0.0292 - layer_2_accuracy: 0.0000e+00 - layer_3_accuracy: 0.1048 - logit_accuracy: 0.8404 - output_accuracy: 0.7497 - val_loss: 0.7474 - val_logit_loss: 0.7474 - val_layer_1_accuracy: 0.0316 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1003 - val_logit_accuracy: 0.8283 - val_output_accuracy: 0.7349\n",
      "Epoch 37/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.4286 - logit_loss: 1.4286 - layer_1_accuracy: 0.0255 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.1185 - logit_accuracy: 0.8117 - output_accuracy: 0.7098 - val_loss: 2.5870 - val_logit_loss: 2.5870 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0549 - val_logit_accuracy: 0.6305 - val_output_accuracy: 0.6813\n",
      "Epoch 38/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 2.3235 - logit_loss: 2.3235 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 1.5272e-04 - layer_3_accuracy: 0.0741 - logit_accuracy: 0.7239 - output_accuracy: 0.6941 - val_loss: 3.7853 - val_logit_loss: 3.7853 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0288 - val_logit_accuracy: 0.6552 - val_output_accuracy: 0.5852\n",
      "Epoch 39/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 2.3084 - logit_loss: 2.3084 - layer_1_accuracy: 0.0121 - layer_2_accuracy: 6.1087e-04 - layer_3_accuracy: 0.0241 - logit_accuracy: 0.7301 - output_accuracy: 0.6822 - val_loss: 1.7324 - val_logit_loss: 1.7324 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0275 - val_logit_accuracy: 0.7541 - val_output_accuracy: 0.6511\n",
      "Epoch 40/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.4823 - logit_loss: 1.4823 - layer_1_accuracy: 0.0096 - layer_2_accuracy: 0.0015 - layer_3_accuracy: 0.0260 - logit_accuracy: 0.7694 - output_accuracy: 0.6698 - val_loss: 1.2950 - val_logit_loss: 1.2950 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0316 - val_logit_accuracy: 0.7624 - val_output_accuracy: 0.7500\n",
      "Epoch 41/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.9480 - logit_loss: 0.9480 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0015 - layer_3_accuracy: 0.0261 - logit_accuracy: 0.8128 - output_accuracy: 0.7639 - val_loss: 0.9274 - val_logit_loss: 0.9274 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0357 - val_logit_accuracy: 0.8118 - val_output_accuracy: 0.7335\n",
      "Epoch 42/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7044 - logit_loss: 0.7044 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0261 - logit_accuracy: 0.8335 - output_accuracy: 0.7308 - val_loss: 0.7379 - val_logit_loss: 0.7379 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0343 - val_logit_accuracy: 0.8022 - val_output_accuracy: 0.6882\n",
      "Epoch 43/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5117 - logit_loss: 0.5117 - layer_1_accuracy: 0.0131 - layer_2_accuracy: 0.0012 - layer_3_accuracy: 0.0296 - logit_accuracy: 0.8554 - output_accuracy: 0.7243 - val_loss: 0.6671 - val_logit_loss: 0.6671 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0412 - val_logit_accuracy: 0.8393 - val_output_accuracy: 0.7129\n",
      "Epoch 44/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4585 - logit_loss: 0.4585 - layer_1_accuracy: 0.0134 - layer_2_accuracy: 0.0014 - layer_3_accuracy: 0.0330 - logit_accuracy: 0.8690 - output_accuracy: 0.7460 - val_loss: 0.6652 - val_logit_loss: 0.6652 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0426 - val_logit_accuracy: 0.8434 - val_output_accuracy: 0.7157\n",
      "Epoch 45/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4389 - logit_loss: 0.4389 - layer_1_accuracy: 0.0137 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0362 - logit_accuracy: 0.8775 - output_accuracy: 0.7508 - val_loss: 0.6559 - val_logit_loss: 0.6559 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0453 - val_logit_accuracy: 0.8544 - val_output_accuracy: 0.7349\n",
      "Epoch 46/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4125 - logit_loss: 0.4125 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0380 - logit_accuracy: 0.8778 - output_accuracy: 0.7567 - val_loss: 0.6226 - val_logit_loss: 0.6226 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0508 - val_logit_accuracy: 0.8503 - val_output_accuracy: 0.7253\n",
      "Epoch 47/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3716 - logit_loss: 0.3716 - layer_1_accuracy: 0.0134 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.0428 - logit_accuracy: 0.8908 - output_accuracy: 0.7637 - val_loss: 0.6368 - val_logit_loss: 0.6368 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0495 - val_logit_accuracy: 0.8571 - val_output_accuracy: 0.7363\n",
      "Epoch 48/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3563 - logit_loss: 0.3563 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0441 - logit_accuracy: 0.8932 - output_accuracy: 0.7841 - val_loss: 0.6105 - val_logit_loss: 0.6105 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0481 - val_logit_accuracy: 0.8558 - val_output_accuracy: 0.7404\n",
      "Epoch 49/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4019 - logit_loss: 0.4019 - layer_1_accuracy: 0.0133 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0493 - logit_accuracy: 0.8729 - output_accuracy: 0.7726 - val_loss: 0.7170 - val_logit_loss: 0.7170 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0495 - val_logit_accuracy: 0.8571 - val_output_accuracy: 0.7253\n",
      "Epoch 50/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3450 - logit_loss: 0.3450 - layer_1_accuracy: 0.0134 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.0502 - logit_accuracy: 0.8923 - output_accuracy: 0.7685 - val_loss: 0.6118 - val_logit_loss: 0.6118 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0577 - val_logit_accuracy: 0.8668 - val_output_accuracy: 0.7527\n",
      "Epoch 51/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3936 - logit_loss: 0.3936 - layer_1_accuracy: 0.0142 - layer_2_accuracy: 0.0017 - layer_3_accuracy: 0.0492 - logit_accuracy: 0.8849 - output_accuracy: 0.7815 - val_loss: 0.5717 - val_logit_loss: 0.5717 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0646 - val_logit_accuracy: 0.8585 - val_output_accuracy: 0.7418\n",
      "Epoch 52/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2977 - logit_loss: 0.2977 - layer_1_accuracy: 0.0142 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0544 - logit_accuracy: 0.9033 - output_accuracy: 0.7805 - val_loss: 0.5725 - val_logit_loss: 0.5725 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0714 - val_logit_accuracy: 0.8393 - val_output_accuracy: 0.8338\n",
      "Epoch 53/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3816 - logit_loss: 0.3816 - layer_1_accuracy: 0.0137 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0687 - logit_accuracy: 0.8867 - output_accuracy: 0.7544 - val_loss: 2.3664 - val_logit_loss: 2.3664 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0975 - val_logit_accuracy: 0.7060 - val_output_accuracy: 0.8214\n",
      "Epoch 54/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 2.7065 - logit_loss: 2.7065 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0113 - layer_3_accuracy: 0.1271 - logit_accuracy: 0.7407 - output_accuracy: 0.7586 - val_loss: 2.1532 - val_logit_loss: 2.1532 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0137 - val_layer_3_accuracy: 0.1415 - val_logit_accuracy: 0.7898 - val_output_accuracy: 0.7555\n",
      "Epoch 55/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.7995 - logit_loss: 1.7995 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0127 - layer_3_accuracy: 0.1374 - logit_accuracy: 0.8044 - output_accuracy: 0.7770 - val_loss: 1.7390 - val_logit_loss: 1.7390 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0124 - val_layer_3_accuracy: 0.1332 - val_logit_accuracy: 0.8036 - val_output_accuracy: 0.7830\n",
      "Epoch 56/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.4958 - logit_loss: 1.4958 - layer_1_accuracy: 0.0128 - layer_2_accuracy: 0.0107 - layer_3_accuracy: 0.1316 - logit_accuracy: 0.8317 - output_accuracy: 0.7694 - val_loss: 1.5017 - val_logit_loss: 1.5017 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0069 - val_layer_3_accuracy: 0.1291 - val_logit_accuracy: 0.8242 - val_output_accuracy: 0.7775\n",
      "Epoch 57/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.4270 - logit_loss: 1.4270 - layer_1_accuracy: 0.0128 - layer_2_accuracy: 0.0087 - layer_3_accuracy: 0.1231 - logit_accuracy: 0.8341 - output_accuracy: 0.7660 - val_loss: 1.4245 - val_logit_loss: 1.4245 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0728 - val_logit_accuracy: 0.8118 - val_output_accuracy: 0.7294\n",
      "Epoch 58/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.1812 - logit_loss: 1.1812 - layer_1_accuracy: 0.0099 - layer_2_accuracy: 0.0037 - layer_3_accuracy: 0.0950 - logit_accuracy: 0.8166 - output_accuracy: 0.7364 - val_loss: 1.0388 - val_logit_loss: 1.0388 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1044 - val_logit_accuracy: 0.8049 - val_output_accuracy: 0.6813\n",
      "Epoch 59/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8822 - logit_loss: 0.8822 - layer_1_accuracy: 0.0102 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.1000 - logit_accuracy: 0.8378 - output_accuracy: 0.7124 - val_loss: 0.8762 - val_logit_loss: 0.8762 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0920 - val_logit_accuracy: 0.8352 - val_output_accuracy: 0.7266\n",
      "Epoch 60/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8147 - logit_loss: 0.8147 - layer_1_accuracy: 0.0110 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.1043 - logit_accuracy: 0.8517 - output_accuracy: 0.7308 - val_loss: 0.8446 - val_logit_loss: 0.8446 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1044 - val_logit_accuracy: 0.8255 - val_output_accuracy: 0.7212\n",
      "Epoch 61/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7901 - logit_loss: 0.7901 - layer_1_accuracy: 0.0110 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.1130 - logit_accuracy: 0.8569 - output_accuracy: 0.7228 - val_loss: 0.8711 - val_logit_loss: 0.8711 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1030 - val_logit_accuracy: 0.8434 - val_output_accuracy: 0.7157\n",
      "Epoch 62/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7745 - logit_loss: 0.7745 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.1069 - logit_accuracy: 0.8650 - output_accuracy: 0.7338 - val_loss: 0.8631 - val_logit_loss: 0.8631 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0975 - val_logit_accuracy: 0.8475 - val_output_accuracy: 0.7253\n",
      "Epoch 63/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7622 - logit_loss: 0.7622 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.1055 - logit_accuracy: 0.8755 - output_accuracy: 0.7407 - val_loss: 0.8573 - val_logit_loss: 0.8573 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0975 - val_logit_accuracy: 0.8558 - val_output_accuracy: 0.7308\n",
      "Epoch 64/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7520 - logit_loss: 0.7520 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.1013 - logit_accuracy: 0.8790 - output_accuracy: 0.7526 - val_loss: 0.8675 - val_logit_loss: 0.8675 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0934 - val_logit_accuracy: 0.8558 - val_output_accuracy: 0.7418\n",
      "Epoch 65/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7419 - logit_loss: 0.7419 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0988 - logit_accuracy: 0.8856 - output_accuracy: 0.7595 - val_loss: 0.8754 - val_logit_loss: 0.8754 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0920 - val_logit_accuracy: 0.8585 - val_output_accuracy: 0.7486\n",
      "Epoch 66/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7335 - logit_loss: 0.7335 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0974 - logit_accuracy: 0.8902 - output_accuracy: 0.7677 - val_loss: 0.8721 - val_logit_loss: 0.8721 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0920 - val_logit_accuracy: 0.8613 - val_output_accuracy: 0.7596\n",
      "Epoch 67/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7261 - logit_loss: 0.7261 - layer_1_accuracy: 0.0139 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0991 - logit_accuracy: 0.8932 - output_accuracy: 0.7750 - val_loss: 0.8517 - val_logit_loss: 0.8517 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0865 - val_logit_accuracy: 0.8668 - val_output_accuracy: 0.7637\n",
      "Epoch 68/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7198 - logit_loss: 0.7198 - layer_1_accuracy: 0.0141 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.0979 - logit_accuracy: 0.8954 - output_accuracy: 0.7845 - val_loss: 0.8497 - val_logit_loss: 0.8497 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0934 - val_logit_accuracy: 0.8668 - val_output_accuracy: 0.7610\n",
      "Epoch 69/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7107 - logit_loss: 0.7107 - layer_1_accuracy: 0.0144 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0979 - logit_accuracy: 0.8984 - output_accuracy: 0.7912 - val_loss: 0.8426 - val_logit_loss: 0.8426 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0893 - val_logit_accuracy: 0.8709 - val_output_accuracy: 0.7706\n",
      "Epoch 70/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7010 - logit_loss: 0.7010 - layer_1_accuracy: 0.0148 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0965 - logit_accuracy: 0.9020 - output_accuracy: 0.7932 - val_loss: 0.8522 - val_logit_loss: 0.8522 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0838 - val_logit_accuracy: 0.8709 - val_output_accuracy: 0.7747\n",
      "Epoch 71/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6965 - logit_loss: 0.6965 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.0959 - logit_accuracy: 0.9039 - output_accuracy: 0.7980 - val_loss: 0.8622 - val_logit_loss: 0.8622 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0879 - val_logit_accuracy: 0.8736 - val_output_accuracy: 0.7775\n",
      "Epoch 72/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6909 - logit_loss: 0.6909 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0959 - logit_accuracy: 0.9070 - output_accuracy: 0.8036 - val_loss: 0.8411 - val_logit_loss: 0.8411 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0879 - val_logit_accuracy: 0.8736 - val_output_accuracy: 0.7830\n",
      "Epoch 73/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6855 - logit_loss: 0.6855 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0965 - logit_accuracy: 0.9070 - output_accuracy: 0.8100 - val_loss: 0.8355 - val_logit_loss: 0.8355 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0852 - val_logit_accuracy: 0.8736 - val_output_accuracy: 0.7843\n",
      "Epoch 74/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6802 - logit_loss: 0.6802 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0982 - logit_accuracy: 0.9087 - output_accuracy: 0.8128 - val_loss: 0.8279 - val_logit_loss: 0.8279 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0865 - val_logit_accuracy: 0.8777 - val_output_accuracy: 0.7885\n",
      "Epoch 75/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6724 - logit_loss: 0.6724 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0982 - logit_accuracy: 0.9105 - output_accuracy: 0.8204 - val_loss: 0.8315 - val_logit_loss: 0.8315 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0975 - val_logit_accuracy: 0.8805 - val_output_accuracy: 0.7885\n",
      "Epoch 76/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6682 - logit_loss: 0.6682 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0988 - logit_accuracy: 0.9128 - output_accuracy: 0.8204 - val_loss: 0.8060 - val_logit_loss: 0.8060 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0934 - val_logit_accuracy: 0.8860 - val_output_accuracy: 0.8022\n",
      "Epoch 77/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6648 - logit_loss: 0.6648 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.0999 - logit_accuracy: 0.9145 - output_accuracy: 0.8270 - val_loss: 0.8185 - val_logit_loss: 0.8185 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0989 - val_logit_accuracy: 0.8832 - val_output_accuracy: 0.8036\n",
      "Epoch 78/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6613 - logit_loss: 0.6613 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0996 - logit_accuracy: 0.9148 - output_accuracy: 0.8306 - val_loss: 0.8143 - val_logit_loss: 0.8143 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1016 - val_logit_accuracy: 0.8846 - val_output_accuracy: 0.8091\n",
      "Epoch 79/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6578 - logit_loss: 0.6578 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.1022 - logit_accuracy: 0.9174 - output_accuracy: 0.8357 - val_loss: 0.8152 - val_logit_loss: 0.8152 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1044 - val_logit_accuracy: 0.8887 - val_output_accuracy: 0.8118\n",
      "Epoch 80/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6542 - logit_loss: 0.6542 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.1028 - logit_accuracy: 0.9194 - output_accuracy: 0.8389 - val_loss: 0.8077 - val_logit_loss: 0.8077 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1044 - val_logit_accuracy: 0.8860 - val_output_accuracy: 0.8118\n",
      "Epoch 81/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6515 - logit_loss: 0.6515 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.1043 - logit_accuracy: 0.9214 - output_accuracy: 0.8436 - val_loss: 0.8054 - val_logit_loss: 0.8054 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1071 - val_logit_accuracy: 0.8956 - val_output_accuracy: 0.8146\n",
      "Epoch 82/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6487 - logit_loss: 0.6487 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.1054 - logit_accuracy: 0.9212 - output_accuracy: 0.8474 - val_loss: 0.8013 - val_logit_loss: 0.8013 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1085 - val_logit_accuracy: 0.8970 - val_output_accuracy: 0.8214\n",
      "Epoch 83/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6462 - logit_loss: 0.6462 - layer_1_accuracy: 0.0153 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.1075 - logit_accuracy: 0.9249 - output_accuracy: 0.8511 - val_loss: 0.8033 - val_logit_loss: 0.8033 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1126 - val_logit_accuracy: 0.8970 - val_output_accuracy: 0.8214\n",
      "Epoch 84/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6436 - logit_loss: 0.6436 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.1078 - logit_accuracy: 0.9273 - output_accuracy: 0.8514 - val_loss: 0.7989 - val_logit_loss: 0.7989 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1154 - val_logit_accuracy: 0.8970 - val_output_accuracy: 0.8283\n",
      "Epoch 85/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6408 - logit_loss: 0.6408 - layer_1_accuracy: 0.0153 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.1097 - logit_accuracy: 0.9288 - output_accuracy: 0.8572 - val_loss: 0.8150 - val_logit_loss: 0.8150 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1154 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.8255\n",
      "Epoch 86/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6386 - logit_loss: 0.6386 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.1122 - logit_accuracy: 0.9302 - output_accuracy: 0.8598 - val_loss: 0.7956 - val_logit_loss: 0.7956 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1223 - val_logit_accuracy: 0.9011 - val_output_accuracy: 0.8297\n",
      "Epoch 87/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6256 - logit_loss: 0.6256 - layer_1_accuracy: 0.0128 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.1138 - logit_accuracy: 0.9093 - output_accuracy: 0.8242 - val_loss: 0.7637 - val_logit_loss: 0.7637 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1085 - val_logit_accuracy: 0.8791 - val_output_accuracy: 0.8077\n",
      "Epoch 88/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5521 - logit_loss: 0.5521 - layer_1_accuracy: 0.0162 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0990 - logit_accuracy: 0.9116 - output_accuracy: 0.8448 - val_loss: 0.7148 - val_logit_loss: 0.7148 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1099 - val_logit_accuracy: 0.8887 - val_output_accuracy: 0.8132\n",
      "Epoch 89/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4883 - logit_loss: 0.4883 - layer_1_accuracy: 0.0180 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.1141 - logit_accuracy: 0.9276 - output_accuracy: 0.8363 - val_loss: 0.7082 - val_logit_loss: 0.7082 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1181 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.8228\n",
      "Epoch 90/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4668 - logit_loss: 0.4668 - layer_1_accuracy: 0.0188 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.1167 - logit_accuracy: 0.9346 - output_accuracy: 0.8471 - val_loss: 0.7067 - val_logit_loss: 0.7067 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1126 - val_logit_accuracy: 0.9025 - val_output_accuracy: 0.8255\n",
      "Epoch 91/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4580 - logit_loss: 0.4580 - layer_1_accuracy: 0.0183 - layer_2_accuracy: 0.0023 - layer_3_accuracy: 0.1184 - logit_accuracy: 0.9386 - output_accuracy: 0.8603 - val_loss: 0.7199 - val_logit_loss: 0.7199 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.1209 - val_logit_accuracy: 0.9038 - val_output_accuracy: 0.8283\n",
      "Epoch 92/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4525 - logit_loss: 0.4525 - layer_1_accuracy: 0.0182 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.1164 - logit_accuracy: 0.9444 - output_accuracy: 0.8676 - val_loss: 0.7083 - val_logit_loss: 0.7083 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1140 - val_logit_accuracy: 0.8997 - val_output_accuracy: 0.8365\n",
      "Epoch 93/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4504 - logit_loss: 0.4504 - layer_1_accuracy: 0.0179 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.1155 - logit_accuracy: 0.9455 - output_accuracy: 0.8745 - val_loss: 0.6952 - val_logit_loss: 0.6952 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1181 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8338\n",
      "Epoch 94/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4575 - logit_loss: 0.4575 - layer_1_accuracy: 0.0194 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.1159 - logit_accuracy: 0.9427 - output_accuracy: 0.8743 - val_loss: 0.7844 - val_logit_loss: 0.7844 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1277 - val_logit_accuracy: 0.8929 - val_output_accuracy: 0.8214\n",
      "Epoch 95/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4525 - logit_loss: 0.4525 - layer_1_accuracy: 0.0217 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.1093 - logit_accuracy: 0.9436 - output_accuracy: 0.8713 - val_loss: 0.6687 - val_logit_loss: 0.6687 - val_layer_1_accuracy: 0.0192 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0975 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.8585\n",
      "Epoch 96/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5015 - logit_loss: 0.5015 - layer_1_accuracy: 0.0211 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.1040 - logit_accuracy: 0.9377 - output_accuracy: 0.8703 - val_loss: 0.6488 - val_logit_loss: 0.6488 - val_layer_1_accuracy: 0.0192 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1058 - val_logit_accuracy: 0.9052 - val_output_accuracy: 0.8489\n",
      "Epoch 97/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4637 - logit_loss: 0.4637 - layer_1_accuracy: 0.0208 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.1055 - logit_accuracy: 0.9455 - output_accuracy: 0.8809 - val_loss: 0.6439 - val_logit_loss: 0.6439 - val_layer_1_accuracy: 0.0192 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1016 - val_logit_accuracy: 0.8956 - val_output_accuracy: 0.8503\n",
      "Epoch 98/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4592 - logit_loss: 0.4592 - layer_1_accuracy: 0.0199 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.1052 - logit_accuracy: 0.9490 - output_accuracy: 0.8827 - val_loss: 0.6791 - val_logit_loss: 0.6791 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1126 - val_logit_accuracy: 0.9038 - val_output_accuracy: 0.8434\n",
      "Epoch 99/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4562 - logit_loss: 0.4562 - layer_1_accuracy: 0.0192 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.1054 - logit_accuracy: 0.9493 - output_accuracy: 0.8890 - val_loss: 0.6654 - val_logit_loss: 0.6654 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1085 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8503\n",
      "Epoch 100/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4519 - logit_loss: 0.4519 - layer_1_accuracy: 0.0192 - layer_2_accuracy: 0.0037 - layer_3_accuracy: 0.1075 - logit_accuracy: 0.9498 - output_accuracy: 0.8916 - val_loss: 0.6619 - val_logit_loss: 0.6619 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1071 - val_logit_accuracy: 0.9121 - val_output_accuracy: 0.8530\n",
      "Epoch 101/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4510 - logit_loss: 0.4510 - layer_1_accuracy: 0.0189 - layer_2_accuracy: 0.0037 - layer_3_accuracy: 0.1046 - logit_accuracy: 0.9519 - output_accuracy: 0.8952 - val_loss: 0.6830 - val_logit_loss: 0.6830 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1113 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8489\n",
      "Epoch 102/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4483 - logit_loss: 0.4483 - layer_1_accuracy: 0.0186 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.1068 - logit_accuracy: 0.9520 - output_accuracy: 0.8972 - val_loss: 0.6617 - val_logit_loss: 0.6617 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1168 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8558\n",
      "Epoch 103/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4471 - logit_loss: 0.4471 - layer_1_accuracy: 0.0185 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.1090 - logit_accuracy: 0.9533 - output_accuracy: 0.8989 - val_loss: 0.6822 - val_logit_loss: 0.6822 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1044 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8503\n",
      "Epoch 104/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4445 - logit_loss: 0.4445 - layer_1_accuracy: 0.0180 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.1025 - logit_accuracy: 0.9530 - output_accuracy: 0.9007 - val_loss: 0.6861 - val_logit_loss: 0.6861 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1071 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8544\n",
      "Epoch 105/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4416 - logit_loss: 0.4416 - layer_1_accuracy: 0.0176 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.1035 - logit_accuracy: 0.9540 - output_accuracy: 0.9044 - val_loss: 0.6563 - val_logit_loss: 0.6563 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1154 - val_logit_accuracy: 0.9107 - val_output_accuracy: 0.8613\n",
      "Epoch 106/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4410 - logit_loss: 0.4410 - layer_1_accuracy: 0.0171 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.1063 - logit_accuracy: 0.9536 - output_accuracy: 0.9082 - val_loss: 0.6451 - val_logit_loss: 0.6451 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1168 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8626\n",
      "Epoch 107/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4405 - logit_loss: 0.4405 - layer_1_accuracy: 0.0166 - layer_2_accuracy: 0.0044 - layer_3_accuracy: 0.1066 - logit_accuracy: 0.9539 - output_accuracy: 0.9081 - val_loss: 0.6579 - val_logit_loss: 0.6579 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1195 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8613\n",
      "Epoch 108/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4375 - logit_loss: 0.4375 - layer_1_accuracy: 0.0170 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.1051 - logit_accuracy: 0.9549 - output_accuracy: 0.9116 - val_loss: 0.6616 - val_logit_loss: 0.6616 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1181 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8585\n",
      "Epoch 109/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4365 - logit_loss: 0.4365 - layer_1_accuracy: 0.0163 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.1066 - logit_accuracy: 0.9543 - output_accuracy: 0.9151 - val_loss: 0.6722 - val_logit_loss: 0.6722 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1126 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8654\n",
      "Epoch 110/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4367 - logit_loss: 0.4367 - layer_1_accuracy: 0.0166 - layer_2_accuracy: 0.0046 - layer_3_accuracy: 0.1074 - logit_accuracy: 0.9540 - output_accuracy: 0.9140 - val_loss: 0.6754 - val_logit_loss: 0.6754 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.1168 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8640\n",
      "Epoch 111/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4957 - logit_loss: 0.4957 - layer_1_accuracy: 0.0165 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.1025 - logit_accuracy: 0.9436 - output_accuracy: 0.9087 - val_loss: 2.0255 - val_logit_loss: 2.0255 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.1113 - val_logit_accuracy: 0.8063 - val_output_accuracy: 0.7349\n",
      "Epoch 112/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.2520 - logit_loss: 1.2520 - layer_1_accuracy: 0.0249 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.0903 - logit_accuracy: 0.8433 - output_accuracy: 0.7836 - val_loss: 1.8179 - val_logit_loss: 1.8179 - val_layer_1_accuracy: 0.0220 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0838 - val_logit_accuracy: 0.7692 - val_output_accuracy: 0.7610\n",
      "Epoch 113/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.9178 - logit_loss: 0.9178 - layer_1_accuracy: 0.0220 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0597 - logit_accuracy: 0.8662 - output_accuracy: 0.8019 - val_loss: 1.1277 - val_logit_loss: 1.1277 - val_layer_1_accuracy: 0.0151 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0563 - val_logit_accuracy: 0.8626 - val_output_accuracy: 0.7637\n",
      "Epoch 114/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6120 - logit_loss: 0.6120 - layer_1_accuracy: 0.0182 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0608 - logit_accuracy: 0.9042 - output_accuracy: 0.8254 - val_loss: 0.9401 - val_logit_loss: 0.9401 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0563 - val_logit_accuracy: 0.8723 - val_output_accuracy: 0.8008\n",
      "Epoch 115/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6469 - logit_loss: 0.6469 - layer_1_accuracy: 0.0160 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.0541 - logit_accuracy: 0.8955 - output_accuracy: 0.8471 - val_loss: 1.2445 - val_logit_loss: 1.2445 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0563 - val_logit_accuracy: 0.8407 - val_output_accuracy: 0.8132\n",
      "Epoch 116/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6096 - logit_loss: 0.6096 - layer_1_accuracy: 0.0168 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0603 - logit_accuracy: 0.9094 - output_accuracy: 0.8308 - val_loss: 0.8229 - val_logit_loss: 0.8229 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0701 - val_logit_accuracy: 0.8832 - val_output_accuracy: 0.8118\n",
      "Epoch 117/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5146 - logit_loss: 0.5146 - layer_1_accuracy: 0.0177 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.0677 - logit_accuracy: 0.9233 - output_accuracy: 0.8537 - val_loss: 0.7630 - val_logit_loss: 0.7630 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0728 - val_logit_accuracy: 0.8764 - val_output_accuracy: 0.8434\n",
      "Epoch 118/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4629 - logit_loss: 0.4629 - layer_1_accuracy: 0.0176 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0646 - logit_accuracy: 0.9307 - output_accuracy: 0.8560 - val_loss: 0.7098 - val_logit_loss: 0.7098 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.8956 - val_output_accuracy: 0.8324\n",
      "Epoch 119/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4371 - logit_loss: 0.4371 - layer_1_accuracy: 0.0179 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.0680 - logit_accuracy: 0.9366 - output_accuracy: 0.8645 - val_loss: 0.7352 - val_logit_loss: 0.7352 - val_layer_1_accuracy: 0.0137 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0701 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.8283\n",
      "Epoch 120/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4489 - logit_loss: 0.4489 - layer_1_accuracy: 0.0174 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0660 - logit_accuracy: 0.9411 - output_accuracy: 0.8705 - val_loss: 0.6876 - val_logit_loss: 0.6876 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0673 - val_logit_accuracy: 0.8984 - val_output_accuracy: 0.8462\n",
      "Epoch 121/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4384 - logit_loss: 0.4384 - layer_1_accuracy: 0.0171 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0651 - logit_accuracy: 0.9427 - output_accuracy: 0.8775 - val_loss: 0.7150 - val_logit_loss: 0.7150 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9107 - val_output_accuracy: 0.8365\n",
      "Epoch 122/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4127 - logit_loss: 0.4127 - layer_1_accuracy: 0.0171 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0643 - logit_accuracy: 0.9455 - output_accuracy: 0.8855 - val_loss: 0.7219 - val_logit_loss: 0.7219 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8393\n",
      "Epoch 123/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4059 - logit_loss: 0.4059 - layer_1_accuracy: 0.0168 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0643 - logit_accuracy: 0.9504 - output_accuracy: 0.8887 - val_loss: 0.7041 - val_logit_loss: 0.7041 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0728 - val_logit_accuracy: 0.9025 - val_output_accuracy: 0.8420\n",
      "Epoch 124/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4009 - logit_loss: 0.4009 - layer_1_accuracy: 0.0168 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0641 - logit_accuracy: 0.9496 - output_accuracy: 0.8992 - val_loss: 0.7028 - val_logit_loss: 0.7028 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8503\n",
      "Epoch 125/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4003 - logit_loss: 0.4003 - layer_1_accuracy: 0.0168 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0643 - logit_accuracy: 0.9527 - output_accuracy: 0.8966 - val_loss: 0.6844 - val_logit_loss: 0.6844 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9052 - val_output_accuracy: 0.8558\n",
      "Epoch 126/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3959 - logit_loss: 0.3959 - layer_1_accuracy: 0.0165 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0644 - logit_accuracy: 0.9511 - output_accuracy: 0.9007 - val_loss: 0.6103 - val_logit_loss: 0.6103 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8695\n",
      "Epoch 127/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4435 - logit_loss: 0.4435 - layer_1_accuracy: 0.0162 - layer_2_accuracy: 0.0037 - layer_3_accuracy: 0.0687 - logit_accuracy: 0.9407 - output_accuracy: 0.9065 - val_loss: 1.0116 - val_logit_loss: 1.0116 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0838 - val_logit_accuracy: 0.8901 - val_output_accuracy: 0.8173\n",
      "Epoch 128/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4118 - logit_loss: 0.4118 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.0825 - logit_accuracy: 0.9414 - output_accuracy: 0.8749 - val_loss: 0.7434 - val_logit_loss: 0.7434 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0879 - val_logit_accuracy: 0.9038 - val_output_accuracy: 0.8489\n",
      "Epoch 129/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3787 - logit_loss: 0.3787 - layer_1_accuracy: 0.0124 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.0832 - logit_accuracy: 0.9531 - output_accuracy: 0.8962 - val_loss: 0.6332 - val_logit_loss: 0.6332 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0865 - val_logit_accuracy: 0.9162 - val_output_accuracy: 0.8558\n",
      "Epoch 130/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3679 - logit_loss: 0.3679 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0817 - logit_accuracy: 0.9537 - output_accuracy: 0.9023 - val_loss: 0.6264 - val_logit_loss: 0.6264 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0852 - val_logit_accuracy: 0.9190 - val_output_accuracy: 0.8571\n",
      "Epoch 131/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3639 - logit_loss: 0.3639 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.0799 - logit_accuracy: 0.9562 - output_accuracy: 0.9064 - val_loss: 0.6157 - val_logit_loss: 0.6157 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0838 - val_logit_accuracy: 0.9245 - val_output_accuracy: 0.8654\n",
      "Epoch 132/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3633 - logit_loss: 0.3633 - layer_1_accuracy: 0.0130 - layer_2_accuracy: 0.0038 - layer_3_accuracy: 0.0793 - logit_accuracy: 0.9565 - output_accuracy: 0.9120 - val_loss: 0.6267 - val_logit_loss: 0.6267 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0838 - val_logit_accuracy: 0.9217 - val_output_accuracy: 0.8640\n",
      "Epoch 133/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3588 - logit_loss: 0.3588 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.0735 - logit_accuracy: 0.9588 - output_accuracy: 0.9137 - val_loss: 0.6353 - val_logit_loss: 0.6353 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0783 - val_logit_accuracy: 0.9121 - val_output_accuracy: 0.8626\n",
      "Epoch 134/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3570 - logit_loss: 0.3570 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0759 - logit_accuracy: 0.9615 - output_accuracy: 0.9177 - val_loss: 0.6470 - val_logit_loss: 0.6470 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0810 - val_logit_accuracy: 0.9107 - val_output_accuracy: 0.8613\n",
      "Epoch 135/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3569 - logit_loss: 0.3569 - layer_1_accuracy: 0.0118 - layer_2_accuracy: 0.0041 - layer_3_accuracy: 0.0751 - logit_accuracy: 0.9618 - output_accuracy: 0.9203 - val_loss: 0.6485 - val_logit_loss: 0.6485 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0783 - val_logit_accuracy: 0.9190 - val_output_accuracy: 0.8654\n",
      "Epoch 136/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3558 - logit_loss: 0.3558 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.0751 - logit_accuracy: 0.9615 - output_accuracy: 0.9218 - val_loss: 0.6101 - val_logit_loss: 0.6101 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0769 - val_logit_accuracy: 0.9148 - val_output_accuracy: 0.8695\n",
      "Epoch 137/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3567 - logit_loss: 0.3567 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0040 - layer_3_accuracy: 0.0733 - logit_accuracy: 0.9621 - output_accuracy: 0.9244 - val_loss: 0.6865 - val_logit_loss: 0.6865 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0852 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8626\n",
      "Epoch 138/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3776 - logit_loss: 0.3776 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.0754 - logit_accuracy: 0.9603 - output_accuracy: 0.9197 - val_loss: 0.7052 - val_logit_loss: 0.7052 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8599\n",
      "Epoch 139/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3915 - logit_loss: 0.3915 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0037 - layer_3_accuracy: 0.0751 - logit_accuracy: 0.9591 - output_accuracy: 0.9203 - val_loss: 0.7420 - val_logit_loss: 0.7420 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0755 - val_logit_accuracy: 0.9121 - val_output_accuracy: 0.8681\n",
      "Epoch 140/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3740 - logit_loss: 0.3740 - layer_1_accuracy: 0.0121 - layer_2_accuracy: 0.0044 - layer_3_accuracy: 0.0739 - logit_accuracy: 0.9615 - output_accuracy: 0.9285 - val_loss: 0.6794 - val_logit_loss: 0.6794 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0755 - val_logit_accuracy: 0.9231 - val_output_accuracy: 0.8709\n",
      "Epoch 141/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4362 - logit_loss: 0.4362 - layer_1_accuracy: 0.0124 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.0677 - logit_accuracy: 0.9511 - output_accuracy: 0.9052 - val_loss: 1.6646 - val_logit_loss: 1.6646 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0357 - val_logit_accuracy: 0.8297 - val_output_accuracy: 0.7321\n",
      "Epoch 142/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7117 - logit_loss: 0.7117 - layer_1_accuracy: 0.0180 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0446 - logit_accuracy: 0.8931 - output_accuracy: 0.8374 - val_loss: 0.8840 - val_logit_loss: 0.8840 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0426 - val_logit_accuracy: 0.8750 - val_output_accuracy: 0.7926\n",
      "Epoch 143/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4836 - logit_loss: 0.4836 - layer_1_accuracy: 0.0220 - layer_2_accuracy: 0.0014 - layer_3_accuracy: 0.0574 - logit_accuracy: 0.9331 - output_accuracy: 0.8661 - val_loss: 0.6529 - val_logit_loss: 0.6529 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0563 - val_logit_accuracy: 0.8970 - val_output_accuracy: 0.8173\n",
      "Epoch 144/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3982 - logit_loss: 0.3982 - layer_1_accuracy: 0.0228 - layer_2_accuracy: 0.0014 - layer_3_accuracy: 0.0599 - logit_accuracy: 0.9479 - output_accuracy: 0.8868 - val_loss: 0.5718 - val_logit_loss: 0.5718 - val_layer_1_accuracy: 0.0192 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0536 - val_logit_accuracy: 0.9121 - val_output_accuracy: 0.8448\n",
      "Epoch 145/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3724 - logit_loss: 0.3724 - layer_1_accuracy: 0.0220 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0594 - logit_accuracy: 0.9530 - output_accuracy: 0.9030 - val_loss: 0.5460 - val_logit_loss: 0.5460 - val_layer_1_accuracy: 0.0179 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0536 - val_logit_accuracy: 0.9135 - val_output_accuracy: 0.8434\n",
      "Epoch 146/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3565 - logit_loss: 0.3565 - layer_1_accuracy: 0.0218 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0606 - logit_accuracy: 0.9565 - output_accuracy: 0.9128 - val_loss: 0.5238 - val_logit_loss: 0.5238 - val_layer_1_accuracy: 0.0165 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0577 - val_logit_accuracy: 0.9190 - val_output_accuracy: 0.8544\n",
      "Epoch 147/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3494 - logit_loss: 0.3494 - layer_1_accuracy: 0.0218 - layer_2_accuracy: 0.0023 - layer_3_accuracy: 0.0612 - logit_accuracy: 0.9594 - output_accuracy: 0.9192 - val_loss: 0.5255 - val_logit_loss: 0.5255 - val_layer_1_accuracy: 0.0206 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0618 - val_logit_accuracy: 0.9258 - val_output_accuracy: 0.8516\n",
      "Epoch 148/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.2167 - logit_loss: 1.2167 - layer_1_accuracy: 0.0220 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0519 - logit_accuracy: 0.8673 - output_accuracy: 0.8334 - val_loss: 1.5839 - val_logit_loss: 1.5839 - val_layer_1_accuracy: 0.0247 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0247 - val_logit_accuracy: 0.7981 - val_output_accuracy: 0.7885\n",
      "Epoch 149/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.1765 - logit_loss: 1.1765 - layer_1_accuracy: 0.0237 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0273 - logit_accuracy: 0.8458 - output_accuracy: 0.7978 - val_loss: 0.9168 - val_logit_loss: 0.9168 - val_layer_1_accuracy: 0.0192 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0220 - val_logit_accuracy: 0.8750 - val_output_accuracy: 0.8159\n",
      "Epoch 150/200\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6752 - logit_loss: 0.6752 - layer_1_accuracy: 0.0218 - layer_2_accuracy: 0.0015 - layer_3_accuracy: 0.0279 - logit_accuracy: 0.8910 - output_accuracy: 0.8451 - val_loss: 0.8549 - val_logit_loss: 0.8549 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0302 - val_logit_accuracy: 0.8681 - val_output_accuracy: 0.7843\n",
      "Epoch 151/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7021 - logit_loss: 0.7021 - layer_1_accuracy: 0.0214 - layer_2_accuracy: 6.1087e-04 - layer_3_accuracy: 0.0386 - logit_accuracy: 0.8829 - output_accuracy: 0.8309 - val_loss: 0.8841 - val_logit_loss: 0.8841 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0316 - val_logit_accuracy: 0.8942 - val_output_accuracy: 0.7940\n",
      "Epoch 152/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6013 - logit_loss: 0.6013 - layer_1_accuracy: 0.0159 - layer_2_accuracy: 7.6359e-04 - layer_3_accuracy: 0.0435 - logit_accuracy: 0.9059 - output_accuracy: 0.8410 - val_loss: 0.7044 - val_logit_loss: 0.7044 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0000e+00 - val_layer_3_accuracy: 0.0316 - val_logit_accuracy: 0.8915 - val_output_accuracy: 0.7953\n",
      "Epoch 153/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5831 - logit_loss: 0.5831 - layer_1_accuracy: 0.0136 - layer_2_accuracy: 4.5816e-04 - layer_3_accuracy: 0.0409 - logit_accuracy: 0.9027 - output_accuracy: 0.8447 - val_loss: 0.6916 - val_logit_loss: 0.6916 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0440 - val_logit_accuracy: 0.8860 - val_output_accuracy: 0.8310\n",
      "Epoch 154/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4774 - logit_loss: 0.4774 - layer_1_accuracy: 0.0142 - layer_2_accuracy: 0.0015 - layer_3_accuracy: 0.0386 - logit_accuracy: 0.9217 - output_accuracy: 0.8407 - val_loss: 0.5772 - val_logit_loss: 0.5772 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0302 - val_logit_accuracy: 0.9052 - val_output_accuracy: 0.8255\n",
      "Epoch 155/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4264 - logit_loss: 0.4264 - layer_1_accuracy: 0.0157 - layer_2_accuracy: 0.0017 - layer_3_accuracy: 0.0403 - logit_accuracy: 0.9337 - output_accuracy: 0.8618 - val_loss: 0.5819 - val_logit_loss: 0.5819 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0330 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8489\n",
      "Epoch 156/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3925 - logit_loss: 0.3925 - layer_1_accuracy: 0.0151 - layer_2_accuracy: 0.0018 - layer_3_accuracy: 0.0458 - logit_accuracy: 0.9333 - output_accuracy: 0.8622 - val_loss: 0.5411 - val_logit_loss: 0.5411 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0371 - val_logit_accuracy: 0.9080 - val_output_accuracy: 0.8407\n",
      "Epoch 157/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3253 - logit_loss: 0.3253 - layer_1_accuracy: 0.0160 - layer_2_accuracy: 0.0020 - layer_3_accuracy: 0.0470 - logit_accuracy: 0.9424 - output_accuracy: 0.8700 - val_loss: 0.5202 - val_logit_loss: 0.5202 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0357 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8448\n",
      "Epoch 158/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3021 - logit_loss: 0.3021 - layer_1_accuracy: 0.0165 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0472 - logit_accuracy: 0.9493 - output_accuracy: 0.8824 - val_loss: 0.4901 - val_logit_loss: 0.4901 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0385 - val_logit_accuracy: 0.9093 - val_output_accuracy: 0.8462\n",
      "Epoch 159/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2936 - logit_loss: 0.2936 - layer_1_accuracy: 0.0162 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0467 - logit_accuracy: 0.9534 - output_accuracy: 0.8885 - val_loss: 0.4994 - val_logit_loss: 0.4994 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0371 - val_logit_accuracy: 0.9190 - val_output_accuracy: 0.8544\n",
      "Epoch 160/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2870 - logit_loss: 0.2870 - layer_1_accuracy: 0.0163 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0458 - logit_accuracy: 0.9571 - output_accuracy: 0.8955 - val_loss: 0.4629 - val_logit_loss: 0.4629 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0398 - val_logit_accuracy: 0.9190 - val_output_accuracy: 0.8571\n",
      "Epoch 161/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2812 - logit_loss: 0.2812 - layer_1_accuracy: 0.0162 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0464 - logit_accuracy: 0.9562 - output_accuracy: 0.8995 - val_loss: 0.5395 - val_logit_loss: 0.5395 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0385 - val_logit_accuracy: 0.9121 - val_output_accuracy: 0.8544\n",
      "Epoch 162/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3466 - logit_loss: 0.3466 - layer_1_accuracy: 0.0157 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0435 - logit_accuracy: 0.9469 - output_accuracy: 0.9068 - val_loss: 0.8734 - val_logit_loss: 0.8734 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0234 - val_logit_accuracy: 0.8860 - val_output_accuracy: 0.8091\n",
      "Epoch 163/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4676 - logit_loss: 0.4676 - layer_1_accuracy: 0.0116 - layer_2_accuracy: 0.0023 - layer_3_accuracy: 0.0467 - logit_accuracy: 0.9299 - output_accuracy: 0.8664 - val_loss: 0.6343 - val_logit_loss: 0.6343 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0467 - val_logit_accuracy: 0.8997 - val_output_accuracy: 0.8269\n",
      "Epoch 164/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3042 - logit_loss: 0.3042 - layer_1_accuracy: 0.0128 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0483 - logit_accuracy: 0.9517 - output_accuracy: 0.8914 - val_loss: 0.5075 - val_logit_loss: 0.5075 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0522 - val_logit_accuracy: 0.9176 - val_output_accuracy: 0.8709\n",
      "Epoch 165/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2853 - logit_loss: 0.2853 - layer_1_accuracy: 0.0150 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0512 - logit_accuracy: 0.9566 - output_accuracy: 0.9007 - val_loss: 0.5152 - val_logit_loss: 0.5152 - val_layer_1_accuracy: 0.0124 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0481 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8626\n",
      "Epoch 166/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3580 - logit_loss: 0.3580 - layer_1_accuracy: 0.0134 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0489 - logit_accuracy: 0.9395 - output_accuracy: 0.8997 - val_loss: 0.4416 - val_logit_loss: 0.4416 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0385 - val_logit_accuracy: 0.8997 - val_output_accuracy: 0.8777\n",
      "Epoch 167/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2995 - logit_loss: 0.2995 - layer_1_accuracy: 0.0136 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0476 - logit_accuracy: 0.9540 - output_accuracy: 0.9053 - val_loss: 0.4266 - val_logit_loss: 0.4266 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0440 - val_logit_accuracy: 0.9107 - val_output_accuracy: 0.8723\n",
      "Epoch 168/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2796 - logit_loss: 0.2796 - layer_1_accuracy: 0.0141 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0582 - logit_accuracy: 0.9569 - output_accuracy: 0.9036 - val_loss: 0.4848 - val_logit_loss: 0.4848 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0604 - val_logit_accuracy: 0.9135 - val_output_accuracy: 0.8750\n",
      "Epoch 169/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3423 - logit_loss: 0.3423 - layer_1_accuracy: 0.0145 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0666 - logit_accuracy: 0.9392 - output_accuracy: 0.8819 - val_loss: 0.6313 - val_logit_loss: 0.6313 - val_layer_1_accuracy: 0.0110 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0646 - val_logit_accuracy: 0.9038 - val_output_accuracy: 0.8379\n",
      "Epoch 170/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3504 - logit_loss: 0.3504 - layer_1_accuracy: 0.0154 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0586 - logit_accuracy: 0.9342 - output_accuracy: 0.8823 - val_loss: 0.5653 - val_logit_loss: 0.5653 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0536 - val_logit_accuracy: 0.9066 - val_output_accuracy: 0.8379\n",
      "Epoch 171/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2997 - logit_loss: 0.2997 - layer_1_accuracy: 0.0121 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.0672 - logit_accuracy: 0.9527 - output_accuracy: 0.8870 - val_loss: 0.5402 - val_logit_loss: 0.5402 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0632 - val_logit_accuracy: 0.9148 - val_output_accuracy: 0.8448\n",
      "Epoch 172/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2667 - logit_loss: 0.2667 - layer_1_accuracy: 0.0144 - layer_2_accuracy: 0.0038 - layer_3_accuracy: 0.0695 - logit_accuracy: 0.9551 - output_accuracy: 0.9033 - val_loss: 0.5064 - val_logit_loss: 0.5064 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9162 - val_output_accuracy: 0.8558\n",
      "Epoch 173/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2365 - logit_loss: 0.2365 - layer_1_accuracy: 0.0131 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0751 - logit_accuracy: 0.9597 - output_accuracy: 0.9026 - val_loss: 0.4995 - val_logit_loss: 0.4995 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9203 - val_output_accuracy: 0.8558\n",
      "Epoch 174/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2242 - logit_loss: 0.2242 - layer_1_accuracy: 0.0127 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0722 - logit_accuracy: 0.9652 - output_accuracy: 0.9120 - val_loss: 0.4493 - val_logit_loss: 0.4493 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9286 - val_output_accuracy: 0.8640\n",
      "Epoch 175/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2171 - logit_loss: 0.2171 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0718 - logit_accuracy: 0.9675 - output_accuracy: 0.9191 - val_loss: 0.4286 - val_logit_loss: 0.4286 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9245 - val_output_accuracy: 0.8736\n",
      "Epoch 176/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2139 - logit_loss: 0.2139 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0693 - logit_accuracy: 0.9659 - output_accuracy: 0.9214 - val_loss: 0.4699 - val_logit_loss: 0.4699 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0714 - val_logit_accuracy: 0.9258 - val_output_accuracy: 0.8709\n",
      "Epoch 177/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2071 - logit_loss: 0.2071 - layer_1_accuracy: 0.0124 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0693 - logit_accuracy: 0.9679 - output_accuracy: 0.9261 - val_loss: 0.4391 - val_logit_loss: 0.4391 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9258 - val_output_accuracy: 0.8777\n",
      "Epoch 178/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2014 - logit_loss: 0.2014 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0677 - logit_accuracy: 0.9681 - output_accuracy: 0.9328 - val_loss: 0.4703 - val_logit_loss: 0.4703 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0646 - val_logit_accuracy: 0.9272 - val_output_accuracy: 0.8764\n",
      "Epoch 179/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1984 - logit_loss: 0.1984 - layer_1_accuracy: 0.0119 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0686 - logit_accuracy: 0.9696 - output_accuracy: 0.9328 - val_loss: 0.4656 - val_logit_loss: 0.4656 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0755 - val_logit_accuracy: 0.9272 - val_output_accuracy: 0.8805\n",
      "Epoch 180/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1900 - logit_loss: 0.1900 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0707 - logit_accuracy: 0.9699 - output_accuracy: 0.9375 - val_loss: 0.4094 - val_logit_loss: 0.4094 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9299 - val_output_accuracy: 0.8942\n",
      "Epoch 181/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1868 - logit_loss: 0.1868 - layer_1_accuracy: 0.0124 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.0680 - logit_accuracy: 0.9693 - output_accuracy: 0.9365 - val_loss: 0.4527 - val_logit_loss: 0.4527 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0673 - val_logit_accuracy: 0.9286 - val_output_accuracy: 0.8846\n",
      "Epoch 182/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1839 - logit_loss: 0.1839 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0035 - layer_3_accuracy: 0.0664 - logit_accuracy: 0.9702 - output_accuracy: 0.9406 - val_loss: 0.4231 - val_logit_loss: 0.4231 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0041 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9245 - val_output_accuracy: 0.8874\n",
      "Epoch 183/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1936 - logit_loss: 0.1936 - layer_1_accuracy: 0.0127 - layer_2_accuracy: 0.0032 - layer_3_accuracy: 0.0669 - logit_accuracy: 0.9664 - output_accuracy: 0.9441 - val_loss: 0.4623 - val_logit_loss: 0.4623 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9272 - val_output_accuracy: 0.8874\n",
      "Epoch 184/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2381 - logit_loss: 0.2381 - layer_1_accuracy: 0.0113 - layer_2_accuracy: 0.0034 - layer_3_accuracy: 0.0666 - logit_accuracy: 0.9667 - output_accuracy: 0.9262 - val_loss: 0.3323 - val_logit_loss: 0.3323 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9354 - val_output_accuracy: 0.8984\n",
      "Epoch 185/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2538 - logit_loss: 0.2538 - layer_1_accuracy: 0.0111 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0641 - logit_accuracy: 0.9646 - output_accuracy: 0.9333 - val_loss: 0.3420 - val_logit_loss: 0.3420 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9341 - val_output_accuracy: 0.8970\n",
      "Epoch 186/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2427 - logit_loss: 0.2427 - layer_1_accuracy: 0.0113 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0660 - logit_accuracy: 0.9687 - output_accuracy: 0.9417 - val_loss: 0.3544 - val_logit_loss: 0.3544 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0742 - val_logit_accuracy: 0.9354 - val_output_accuracy: 0.9011\n",
      "Epoch 187/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2400 - logit_loss: 0.2400 - layer_1_accuracy: 0.0115 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0664 - logit_accuracy: 0.9679 - output_accuracy: 0.9444 - val_loss: 0.3189 - val_logit_loss: 0.3189 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0714 - val_logit_accuracy: 0.9451 - val_output_accuracy: 0.9066\n",
      "Epoch 188/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3527 - logit_loss: 0.3527 - layer_1_accuracy: 0.0137 - layer_2_accuracy: 0.0021 - layer_3_accuracy: 0.0733 - logit_accuracy: 0.9546 - output_accuracy: 0.9169 - val_loss: 0.5006 - val_logit_loss: 0.5006 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0810 - val_logit_accuracy: 0.9258 - val_output_accuracy: 0.8942\n",
      "Epoch 189/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2815 - logit_loss: 0.2815 - layer_1_accuracy: 0.0133 - layer_2_accuracy: 0.0024 - layer_3_accuracy: 0.0750 - logit_accuracy: 0.9647 - output_accuracy: 0.9322 - val_loss: 0.4002 - val_logit_loss: 0.4002 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0769 - val_logit_accuracy: 0.9341 - val_output_accuracy: 0.9093\n",
      "Epoch 190/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2637 - logit_loss: 0.2637 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0761 - logit_accuracy: 0.9675 - output_accuracy: 0.9417 - val_loss: 0.4041 - val_logit_loss: 0.4041 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0783 - val_logit_accuracy: 0.9368 - val_output_accuracy: 0.9052\n",
      "Epoch 191/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2435 - logit_loss: 0.2435 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0748 - logit_accuracy: 0.9695 - output_accuracy: 0.9369 - val_loss: 0.5350 - val_logit_loss: 0.5350 - val_layer_1_accuracy: 0.0055 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0714 - val_logit_accuracy: 0.9217 - val_output_accuracy: 0.8695\n",
      "Epoch 192/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2706 - logit_loss: 0.2706 - layer_1_accuracy: 0.0111 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.0635 - logit_accuracy: 0.9644 - output_accuracy: 0.9250 - val_loss: 0.3828 - val_logit_loss: 0.3828 - val_layer_1_accuracy: 0.0055 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0673 - val_logit_accuracy: 0.9368 - val_output_accuracy: 0.8942\n",
      "Epoch 193/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2315 - logit_loss: 0.2315 - layer_1_accuracy: 0.0113 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0666 - logit_accuracy: 0.9701 - output_accuracy: 0.9403 - val_loss: 0.3983 - val_logit_loss: 0.3983 - val_layer_1_accuracy: 0.0055 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0701 - val_logit_accuracy: 0.9299 - val_output_accuracy: 0.8997\n",
      "Epoch 194/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2258 - logit_loss: 0.2258 - layer_1_accuracy: 0.0122 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0658 - logit_accuracy: 0.9716 - output_accuracy: 0.9455 - val_loss: 0.3882 - val_logit_loss: 0.3882 - val_layer_1_accuracy: 0.0096 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0728 - val_logit_accuracy: 0.9299 - val_output_accuracy: 0.8956\n",
      "Epoch 195/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2266 - logit_loss: 0.2266 - layer_1_accuracy: 0.0128 - layer_2_accuracy: 0.0026 - layer_3_accuracy: 0.0683 - logit_accuracy: 0.9699 - output_accuracy: 0.9459 - val_loss: 0.3940 - val_logit_loss: 0.3940 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0014 - val_layer_3_accuracy: 0.0701 - val_logit_accuracy: 0.9258 - val_output_accuracy: 0.9011\n",
      "Epoch 196/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2234 - logit_loss: 0.2234 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0031 - layer_3_accuracy: 0.0675 - logit_accuracy: 0.9725 - output_accuracy: 0.9501 - val_loss: 0.3874 - val_logit_loss: 0.3874 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0687 - val_logit_accuracy: 0.9341 - val_output_accuracy: 0.9011\n",
      "Epoch 197/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2217 - logit_loss: 0.2217 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0027 - layer_3_accuracy: 0.0672 - logit_accuracy: 0.9724 - output_accuracy: 0.9525 - val_loss: 0.3733 - val_logit_loss: 0.3733 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9313 - val_output_accuracy: 0.9080\n",
      "Epoch 198/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2226 - logit_loss: 0.2226 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0663 - logit_accuracy: 0.9725 - output_accuracy: 0.9511 - val_loss: 0.3541 - val_logit_loss: 0.3541 - val_layer_1_accuracy: 0.0082 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0673 - val_logit_accuracy: 0.9341 - val_output_accuracy: 0.9093\n",
      "Epoch 199/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2207 - logit_loss: 0.2207 - layer_1_accuracy: 0.0125 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0664 - logit_accuracy: 0.9733 - output_accuracy: 0.9543 - val_loss: 0.3725 - val_logit_loss: 0.3725 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0659 - val_logit_accuracy: 0.9327 - val_output_accuracy: 0.9066\n",
      "Epoch 200/200\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2195 - logit_loss: 0.2195 - layer_1_accuracy: 0.0124 - layer_2_accuracy: 0.0029 - layer_3_accuracy: 0.0661 - logit_accuracy: 0.9734 - output_accuracy: 0.9549 - val_loss: 0.3729 - val_logit_loss: 0.3729 - val_layer_1_accuracy: 0.0069 - val_layer_2_accuracy: 0.0027 - val_layer_3_accuracy: 0.0673 - val_logit_accuracy: 0.9327 - val_output_accuracy: 0.9080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ebad1b83d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 55)]              0         \n",
      "                                                                 \n",
      " layer_1 (Dense)             (None, 64)                3584      \n",
      "                                                                 \n",
      " layer_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " layer_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " logit (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      " output (Activation)         (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,857\n",
      "Trainable params: 9,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NN to ONNX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "# convert the model to ONNX format\n",
    "onnx_net = onnxmltools.convert_keras(model)\n",
    "onnxmltools.utils.save_model(onnx_net, \"my_model.onnx\")\n",
    "content = onnx_net.SerializeToString()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def setup(onnx_file: str,):\n",
    "    # Load the ONNX model\n",
    "    ort_sess = ort.InferenceSession(onnx_file)\n",
    "    ortvalue = ort.OrtValue.ortvalue_from_numpy(X_train.astype(np.float32), 'cpu')\n",
    "    logits = ort_sess.run(['logit'], {ort_sess.get_inputs()[0].name: ortvalue})\n",
    "    outputs = ort_sess.run(['output'], {ort_sess.get_inputs()[0].name: ortvalue})\n",
    "    x_0 = X_train[y_train == 0].astype(np.float32)\n",
    "    x_1 = X_train[y_train == 1].astype(np.float32)\n",
    "\n",
    "    # get predictions for each class with positive true label   \n",
    "    pred_0 = ort_sess.run(['output'], {ort_sess.get_inputs()[0].name: x_0})[0][:,0]\n",
    "    x_0 = x_0[pred_0 < 0.5]\n",
    "    pred_1 = ort_sess.run(['output'], {ort_sess.get_inputs()[0].name: x_1})[0][:,0]\n",
    "    x_1 = x_1[pred_1 > 0.5]\n",
    "\n",
    "    logits_0_correct = ort_sess.run(['logit'], {ort_sess.get_inputs()[0].name: x_0})[0]\n",
    "    logits_1_correct = ort_sess.run(['logit'], {ort_sess.get_inputs()[0].name: x_1})[0]\n",
    "\n",
    "    # fit GMM to class conditional distributions\n",
    "    best_bic = 100000000\n",
    "    best_gmm_0 = GaussianMixture(n_components=1, random_state=0)\n",
    "    for i in range(1,20):\n",
    "        gmm_0 = GaussianMixture(n_components=i, random_state=0).fit(logits_0_correct)\n",
    "        current_bic = gmm_0.bic(logits_0_correct)\n",
    "        if current_bic < best_bic:\n",
    "            best_bic = current_bic\n",
    "            best_gmm_0 = gmm_0\n",
    "\n",
    "    best_bic = 100000000\n",
    "    best_gmm_1 = GaussianMixture(n_components=1, random_state=0)\n",
    "    for i in range(1,20):\n",
    "        gmm_1 = GaussianMixture(n_components=i, random_state=0).fit(logits_1_correct)\n",
    "        current_bic = gmm_1.bic(logits_1_correct)\n",
    "        if current_bic < best_bic:\n",
    "            best_bic = current_bic\n",
    "            best_components = i\n",
    "            best_gmm_1 = gmm_1 \n",
    "    \n",
    "    return  best_gmm_0, best_gmm_1, ort_sess\n",
    "\n",
    "gmm_0, gmm_1, ort_sess = setup('my_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmm_0 number of components is 8\n",
      "gmm_1 number of_components is 13\n"
     ]
    }
   ],
   "source": [
    "print('gmm_0 number of components is {}'.format(gmm_0.get_params()['n_components']))\n",
    "print('gmm_1 number of_components is {}'.format(gmm_1.get_params()['n_components']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "confidence_score: [1.33171219e-01 1.40106994e-01 1.40271733e-01 1.04818548e-07\n",
      " 1.33101336e-01 1.04631875e-07 1.33285274e-01 1.43718172e-01\n",
      " 1.40106994e-01 1.32052336e-01 1.37786237e-01 1.33778437e-01\n",
      " 1.34492419e-07 1.33137731e-01 2.09632884e-07 1.33011194e-01\n",
      " 9.09396011e-08 1.35961599e-01 1.33147226e-01 1.40353751e-01\n",
      " 1.36192200e-01 1.37232426e-01 1.35216869e-01 1.31755119e-01\n",
      " 1.34205883e-01 8.90794597e-08 1.35773394e-01 1.35519600e-01\n",
      " 1.36657753e-01 1.35765964e-01 1.33273178e-01 8.26503522e-08\n",
      " 1.10276497e-07 1.26735974e-07 3.21314960e-07 1.35326437e-01\n",
      " 5.32073450e-07 1.04398745e-07 8.85597901e-08 1.03840474e-07\n",
      " 1.07070677e-07 1.34170628e-07 1.05083420e-07 1.36107985e-01\n",
      " 1.10048822e-07 1.00074832e-07 9.08287982e-08 1.36906653e-01\n",
      " 9.73227350e-08 1.36077609e-01 1.36200280e-01 1.32008543e-01\n",
      " 9.40868079e-08 1.36377242e-01 1.35831459e-01 1.33147226e-01\n",
      " 1.71677572e-01 1.07043477e-07 3.33752349e-07 1.35959041e-01\n",
      " 1.33113280e-01 9.52346791e-08 1.34959548e-01 1.36309416e-01\n",
      " 1.32013439e-01 1.35120152e-01 1.34116635e-01 1.33295026e-01\n",
      " 1.63759297e-01 1.33301963e-01 1.39770814e-01 1.91213559e-07\n",
      " 1.35281429e-01 1.36657753e-01 9.04881681e-08 9.77878956e-08\n",
      " 1.33783413e-01 1.07897535e-07 1.48208500e-01 9.83031230e-08\n",
      " 8.69515918e-08 9.31295757e-08 1.32384676e-01 8.86964808e-08\n",
      " 2.75043632e-07 1.03755250e-07 8.77120920e-08 1.38176050e-01\n",
      " 1.32437701e-01 1.33816259e-07 1.38968362e-01 1.33265617e-01\n",
      " 1.10658073e-07 1.91523425e-07 1.00059379e-07 1.33170224e-01\n",
      " 1.35216957e-01 1.33583191e-01 8.90833922e-08 8.57998228e-08\n",
      " 1.32098091e-01 1.33559877e-01 1.33119671e-01 1.31984199e-01\n",
      " 1.34084252e-01 8.31248102e-08 1.37102586e-07 1.35749054e-01\n",
      " 1.08444460e-07 1.48663779e-07 9.79486437e-08 9.57809914e-08\n",
      " 1.02812061e-07 1.36787573e-07 1.42310429e-07 9.06299823e-08\n",
      " 1.04270997e-07 1.36115953e-01 1.36473874e-01 9.07380674e-08\n",
      " 1.60438247e-01 1.32571461e-01 1.05859278e-07 1.32951535e-01\n",
      " 1.33320246e-01 9.07858635e-08 9.51963197e-08 1.41989171e-01\n",
      " 1.37168084e-01 1.35049891e-01 1.34458454e-01 9.01137893e-08\n",
      " 9.48135624e-08 1.40694501e-01 1.33294162e-01 1.86966721e-07\n",
      " 1.10391537e-07 1.08669486e-07 9.85632777e-08 1.30811980e-07\n",
      " 3.12759859e-07 1.36740594e-01 1.37371157e-01 1.92003742e-07\n",
      " 1.34753280e-01 1.35763763e-01 1.39267835e-01 1.41989171e-01\n",
      " 1.31953662e-01 1.33284528e-01 1.32980452e-01 1.43409859e-01\n",
      " 1.07665739e-07 1.31754375e-01 1.33137731e-01 9.45813401e-08\n",
      " 9.01988593e-08 1.33893152e-01 1.39982771e-01 8.21983489e-08\n",
      " 1.32169111e-01 1.33854394e-01 1.08675187e-07 3.17556966e-07\n",
      " 1.33601200e-01 1.35251841e-01 1.36740594e-01 1.32737377e-01\n",
      " 1.37777545e-01 1.35519600e-01 1.36540248e-01 1.38651371e-01\n",
      " 1.36763703e-01 3.10468437e-07 1.32056577e-01 1.32823708e-01\n",
      " 1.51223951e-01 1.33409083e-01 1.31763390e-01 1.37037911e-07\n",
      " 1.34753280e-01 1.02791674e-07 1.34112685e-01 1.33897795e-01\n",
      " 9.13004304e-08 1.33295026e-01 1.37413891e-01 9.12019981e-08\n",
      " 1.35980986e-07 1.39265575e-01 1.35249585e-01 1.38817497e-01\n",
      " 1.33183547e-01 1.21595303e-07 1.32156946e-01 8.58394096e-08\n",
      " 1.36442336e-01 1.52761512e-01 1.04998452e-07 8.01752280e-07\n",
      " 8.90924529e-08 9.19909876e-08 1.35799019e-01 1.93915420e-07\n",
      " 1.36657753e-01 1.36104046e-01 1.10053396e-07 1.35216869e-01\n",
      " 1.34095353e-01 1.07851920e-07 9.19867596e-08 1.32632369e-01\n",
      " 1.41989171e-01 1.36155363e-01 9.96993582e-08 1.34229017e-01\n",
      " 9.67852469e-08 1.39827549e-01 1.39215833e-01 1.33300592e-01\n",
      " 1.25571053e-07 1.35870804e-01 1.12338604e-07 8.84498658e-08\n",
      " 8.48681111e-08 9.81675370e-08 1.14766763e-07 1.35898470e-01\n",
      " 1.35391342e-01 1.36467098e-01 1.33697586e-01 9.19408086e-08\n",
      " 1.33104351e-01 1.14431106e-07 1.33783413e-01 1.35250086e-01\n",
      " 1.35439959e-01 9.73685336e-08 1.36181524e-01 1.36097293e-07\n",
      " 1.31760035e-01 1.31857166e-01 1.32013439e-01 9.20721655e-08\n",
      " 8.86425679e-08 1.35225005e-01 1.74598398e-07 1.36920526e-07\n",
      " 1.33409083e-01 1.33583191e-01 1.33409083e-01 1.68253623e-01\n",
      " 1.32750112e-01 1.32910041e-07 1.38113402e-01 1.69190050e-07\n",
      " 1.52120510e-01 1.33893152e-01 1.35940573e-01 1.62999142e-01\n",
      " 1.34019065e-01 1.32376944e-07 8.19899111e-08 1.34814342e-07\n",
      " 1.35680377e-01 1.02790980e-07 1.36541409e-01 1.33945477e-01\n",
      " 8.61131336e-08 1.33313208e-07 1.35870444e-01 1.31818873e-01\n",
      " 1.33602611e-01 1.22313143e-07 1.33778341e-07 1.33120422e-01\n",
      " 1.33105205e-01 1.10506874e-07 1.37270100e-01 9.75967323e-08\n",
      " 1.32153191e-01 2.98321171e-07 1.32191354e-01 1.10660180e-07\n",
      " 9.05920641e-08 1.32548501e-01 1.35777866e-01 1.35252567e-01\n",
      " 1.36135506e-01 1.35793660e-01 1.92019539e-07 1.04599607e-07\n",
      " 1.63056962e-01 1.31765605e-01 1.07927056e-07 1.40070217e-01\n",
      " 1.02362103e-07 1.00342265e-07 1.01492368e-07 1.35324659e-01\n",
      " 9.11592999e-08 3.16944166e-07 1.07087747e-07 8.23876201e-07\n",
      " 1.32951535e-01 1.33234189e-01 1.37869924e-07 1.31953662e-01\n",
      " 1.09846239e-07 1.09963450e-07 8.82622531e-08 1.34001283e-01\n",
      " 2.04110147e-07 1.36858606e-07 1.36213883e-01 1.07414037e-07\n",
      " 2.37607798e-07 1.33774979e-01 1.04863614e-07 1.31792190e-01\n",
      " 1.35249585e-01 1.38175643e-01 8.19704665e-08 1.90927515e-07\n",
      " 1.32812210e-01 9.09078922e-08 1.34796723e-01 1.35754828e-01\n",
      " 1.33059074e-07 1.43313314e-01 1.58093944e-01 9.92523492e-08\n",
      " 1.79753679e-07 1.36673482e-01 9.35100612e-08 8.61579899e-08\n",
      " 1.37034619e-01 1.39795666e-01 3.22555208e-07 1.45122176e-07\n",
      " 1.36212472e-01 9.19413356e-08 8.59836149e-08 1.90605823e-07\n",
      " 1.40106994e-01 1.52140097e-07 1.39690821e-01 1.90057713e-07\n",
      " 1.04038233e-07 8.36726840e-08 8.82965627e-08 1.04212441e-07\n",
      " 1.42619786e-01 8.83178584e-08 9.65222170e-08 8.21566643e-08\n",
      " 1.36198253e-01 1.32044167e-01 1.36137789e-01 1.07049496e-07\n",
      " 4.66208662e-07 1.89836434e-07 2.03198846e-07 1.41989171e-01\n",
      " 1.03648012e-07 1.35387281e-01 1.32098091e-01 1.40070217e-01\n",
      " 1.32844182e-01 1.37619244e-01 1.34615237e-07 8.48509938e-08\n",
      " 8.90670121e-08 2.04194494e-07 9.47154905e-08 1.48629849e-01\n",
      " 1.35763279e-01 1.32007126e-01 1.31922489e-01 9.93919757e-08\n",
      " 1.32112293e-01 1.33192877e-01 1.33854394e-01 1.05104633e-07\n",
      " 1.67132077e-01 1.35298097e-01 1.35905469e-01 1.35888234e-07\n",
      " 1.38127997e-01 1.05171537e-07 1.33200161e-07 1.39799585e-01\n",
      " 1.32079069e-01 1.34108508e-01 9.91789849e-08 1.35843908e-01\n",
      " 1.32384676e-01 1.77340991e-07 1.34116659e-01 1.32438720e-01\n",
      " 9.47544013e-08 9.91412771e-08 1.35980466e-01 1.33272122e-01\n",
      " 9.55772468e-08 8.89523192e-08 1.31755119e-01 8.44979502e-08\n",
      " 8.88536812e-08 1.56928160e-01 1.35366562e-01 1.36108850e-01\n",
      " 1.33284528e-01 1.32432860e-01 1.33273178e-01 1.39027464e-01\n",
      " 1.31922489e-01 1.35371175e-01 1.32098091e-01 1.36436671e-01\n",
      " 1.36115953e-01 1.33161232e-01 8.35660061e-08 1.02600245e-07\n",
      " 1.32155172e-01 9.84285743e-08 1.34905364e-01 1.35856105e-01\n",
      " 1.10195992e-07 9.40841913e-08 1.25125676e-07 1.33774555e-01\n",
      " 1.31756974e-01 9.41128119e-08 1.31768189e-01 1.33893152e-01\n",
      " 1.33716603e-07 9.67862112e-08 1.33171219e-01 1.10431633e-07\n",
      " 9.59361966e-08 1.35216869e-01 1.32844182e-01 1.33833287e-01\n",
      " 1.37193303e-01 8.94321518e-08 1.35974686e-01 1.36906653e-01\n",
      " 1.07178335e-07 8.70650973e-08 1.33102684e-01 1.35330195e-01\n",
      " 1.31980680e-01 1.32384676e-01 8.82427878e-08 1.37580971e-01\n",
      " 1.75921961e-07 1.38175643e-01 1.35377244e-01 1.38760280e-07\n",
      " 1.36184043e-01 1.36498715e-01 1.37347912e-01 1.31816550e-01\n",
      " 1.36740594e-01 1.95708121e-07 1.33837193e-01 1.31763390e-01\n",
      " 1.35372538e-01 1.25026333e-07 1.35380973e-01 1.35956362e-01\n",
      " 1.37580971e-01 1.82683563e-07 1.37312051e-01 1.32772909e-01\n",
      " 1.35803849e-01 1.05153458e-07 1.35371175e-01 1.33117226e-01\n",
      " 2.15631677e-07 1.34076150e-01 1.36025063e-01 1.33325959e-01\n",
      " 1.31952180e-01 1.02198012e-07 2.65713316e-07 1.28926240e-07\n",
      " 1.37742361e-01 1.36763703e-01 1.35957067e-01 1.36239790e-01\n",
      " 1.33279074e-01 1.33739081e-01 1.32750112e-01 1.04070086e-07\n",
      " 1.35953250e-01 1.31792190e-01 1.36135299e-01 1.10300321e-07\n",
      " 1.36657753e-01 1.33242329e-01 9.70195425e-08 1.33833287e-01\n",
      " 1.36247497e-01 1.32096801e-01 1.35216093e-01 1.33152901e-01\n",
      " 1.33285274e-01 1.32194845e-01 1.37193303e-01 1.65683623e-01\n",
      " 1.33279074e-01 1.36774795e-01 9.29666991e-08 1.34001283e-01\n",
      " 1.31756242e-01 1.31768189e-01 1.09009834e-07 1.49103861e-07\n",
      " 1.52030224e-07 1.37163579e-01 1.33804597e-01 1.33876093e-01\n",
      " 1.32137131e-01 1.33067414e-07 1.33945477e-01 1.35377244e-01\n",
      " 1.29925228e-07 1.35146537e-07 1.36390769e-01 1.69090517e-01\n",
      " 1.38175643e-01 1.35865343e-07 1.33102815e-01 8.94472045e-08\n",
      " 1.36135299e-01 1.34066939e-01 1.32069179e-01 8.82202237e-08\n",
      " 1.35519600e-01 1.04977544e-07 1.37060912e-07 1.35235183e-01\n",
      " 9.67091157e-08 1.33222835e-01 1.35815544e-01 1.35732782e-01\n",
      " 1.57803554e-01 1.37413891e-01 1.40348261e-07 8.32327703e-08\n",
      " 1.33138210e-01 9.11490066e-08 1.33954204e-01 9.21389278e-08\n",
      " 1.07268368e-07 2.00279359e-07 9.94227216e-08 1.34427889e-01\n",
      " 1.04401705e-07 1.33220135e-01 1.17573225e-07 1.10166295e-07\n",
      " 1.58375447e-01 8.21842780e-08 8.81543158e-08 1.62354245e-07\n",
      " 1.35106869e-01 1.31761804e-01 4.17938713e-05 9.53690991e-08\n",
      " 9.80973625e-08 1.35577914e-01 1.35794908e-01 1.32833589e-01\n",
      " 1.35990508e-01 9.53769212e-08 1.36713503e-07 8.38505187e-08\n",
      " 8.26781851e-08 1.35470990e-01 1.15572663e-07 1.32098091e-01\n",
      " 1.28114960e-07 1.03849325e-07 1.39611748e-01 1.32155172e-01\n",
      " 1.32264406e-01 8.24272357e-08 1.09103007e-07 9.88297318e-08\n",
      " 1.34100467e-01 2.30899136e-07 1.36052536e-01 1.37549894e-01\n",
      " 1.33175751e-01 1.31750957e-01 9.34883588e-08 1.37292782e-07\n",
      " 1.91068601e-07 9.09814153e-08 1.31746732e-01 1.37231772e-07\n",
      " 1.35762775e-01 1.39464335e-01 1.34084252e-01 1.34113426e-01\n",
      " 8.97091887e-07 9.72372697e-08 1.37178806e-01 1.39695162e-01\n",
      " 1.31952180e-01 1.38127997e-01 2.12954501e-07 1.33863344e-01\n",
      " 1.36488632e-01 1.36052536e-01 1.04329632e-07 1.37111448e-07\n",
      " 1.33152901e-01 1.01225828e-07 9.32536453e-08 1.35799019e-01\n",
      " 9.28273642e-08 1.44041948e-07 1.38034175e-01 1.32955457e-07\n",
      " 1.22084205e-07 1.35989940e-01 1.03056249e-07 1.66616752e-07\n",
      " 8.66782913e-08 1.35793660e-01 1.32044167e-01 1.91798019e-07\n",
      " 1.35330195e-01 1.09983269e-07 1.33602611e-01 1.03350958e-07\n",
      " 9.07097582e-08 1.33104351e-01 1.31750957e-01 1.36245024e-01\n",
      " 1.17034059e-07 1.35275519e-01 1.33198908e-01 1.34632768e-01\n",
      " 9.09098644e-08 1.33010026e-01 1.40694501e-01 1.33293421e-01\n",
      " 1.33183180e-01 1.37232426e-01 1.32983879e-01 1.31888315e-01\n",
      " 1.91503269e-07 1.33774555e-01 1.34105383e-01 1.70713329e-01\n",
      " 1.35803921e-01 2.94882071e-07 9.67898078e-08 1.31811626e-01\n",
      " 1.36788754e-01 1.33471009e-07 1.36348677e-07 1.03173380e-07\n",
      " 2.46086124e-07 1.33049373e-07 8.47140418e-08 1.33876093e-01\n",
      " 1.04633301e-07 8.26495344e-08 1.31746658e-01 1.32926835e-07\n",
      " 1.32750112e-01 1.35969619e-07 1.35815544e-01 1.35307396e-01\n",
      " 1.34559110e-07 1.39872458e-01 1.31756242e-01 1.33225497e-01\n",
      " 1.35500020e-01 1.38817497e-01 1.39686385e-01 1.42509069e-01\n",
      " 1.53394996e-07 1.42266644e-01 1.32895041e-01 1.33301963e-01\n",
      " 9.82194702e-08 1.33200161e-07 1.35483844e-01 1.55831332e-01\n",
      " 9.90416029e-08 1.33945477e-01 1.04351524e-07 1.58253745e-01\n",
      " 1.35927513e-07 3.23102801e-07 1.34084252e-01 8.21852274e-08\n",
      " 1.01612723e-07 1.32194845e-01 1.07353786e-07 1.51376351e-01\n",
      " 1.36052536e-01 1.38096792e-01 1.38300108e-01 1.35249585e-01\n",
      " 1.35782616e-01 8.19267967e-08 8.24160279e-08 9.47322086e-08\n",
      " 9.66373646e-08 1.35980466e-01 1.35585503e-07 1.36906653e-01\n",
      " 1.37168084e-01 1.02824064e-07 8.21380870e-08 1.32008543e-01\n",
      " 1.33945477e-01 3.12904217e-07 1.43848076e-01 1.04317345e-07]\n"
     ]
    }
   ],
   "source": [
    "q1 = 80\n",
    "q2 = 60\n",
    "u1 = 0.5\n",
    "u2 = 0.2\n",
    "import math\n",
    "\n",
    "from sympy import Predicate\n",
    "def confidence(x, gmm_0, gmm_1, ort_sess):\n",
    "    logits = ort_sess.run(['logit'], {ort_sess.get_inputs()[0].name: x.astype(np.float32)})[0]\n",
    "    prediction = ort_sess.run([ort_sess.get_outputs()[-1].name], {ort_sess.get_inputs()[0].name: x.astype(np.float32)})[0]\n",
    "    prediction = np.squeeze(np.around(prediction, 0))\n",
    "\n",
    "    u = []\n",
    "    for gmm in [gmm_0, gmm_1]:\n",
    "        p = gmm.score_samples(logits)\n",
    "        s = max(p) - p\n",
    "\n",
    "        sq1 = np.percentile(s, q1)\n",
    "        sq2 = np.percentile(s, q2)\n",
    "\n",
    "        l1 = math.log(1/u1 - 1)\n",
    "        l2 = math.log(1/u2 - 1)\n",
    "        c2 = (sq2 * l1 - sq1 * l2) / (l1 - l2) \n",
    "        c1 = -l2 / (sq2-c2)\n",
    "\n",
    "        g = 1/(1 + np.exp(-c1 * (s - c2)))\n",
    "        u.append(g)\n",
    "    \n",
    "    confidence_score = np.zeros([len(x)])\n",
    "    confidence_score[prediction == 0] = u[0][prediction == 0]\n",
    "    confidence_score[prediction == 1] = u[1][prediction == 1]\n",
    "\n",
    "    return prediction, confidence_score\n",
    "\n",
    "prediction,confidence_score = confidence(X_test, gmm_0, gmm_1, ort_sess)\n",
    "print('prediction:', prediction)\n",
    "print('confidence_score:', confidence_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a46493ef273555f0fac6598162cd73ee5d8ec19f64a4bbbda3cc3aa05bc0ca3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
